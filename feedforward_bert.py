# -*- coding: utf-8 -*-
"""feedforward_bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oWHskCp-0zA92nDmsfAr4bDhMd819Omy
"""

!pip install pymorphy2
import numpy as np
import pandas as pd
import math
from imblearn.over_sampling import RandomOverSampler
import pymorphy2, re

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical

from keras.layers import Input, Embedding, Activation, Flatten, Dense, concatenate
from keras.layers import Conv1D, MaxPooling1D, Dropout, LSTM
from keras.models import Model

from google.colab import drive
drive.mount('/content/drive')

#dataset
PATH_X_TRAIN='/content/drive/My Drive/АиТ/se/sent.csv'
PATH_X_VAL='/content/drive/My Drive/АиТ/se/sent_val.csv'

PATH_Y_TRAIN='/content/drive/My Drive/АиТ/types_train.csv'
PATH_Y_TEST='/content/drive/My Drive/АиТ/types_test.csv'
PATH_Y_VAL='/content/drive/My Drive/АиТ/types_val.csv'

POST_X_TRAIN='/content/drive/My Drive/АиТ/se/post.csv'
POST_X_VAL='/content/drive/My Drive/АиТ/se/post_val.csv'

POST2_X_TRAIN='/content/drive/My Drive/АиТ/se/postpost.csv'
POST2_X_VAL='/content/drive/My Drive/АиТ/se/postpost_val.csv'

POST3_X_TRAIN='/content/drive/My Drive/АиТ/ff_bert/post3_bert_10.csv'
POST3_X_VAL='/content/drive/My Drive/АиТ/ff_bert/post3_bert_10_val.csv'

PRED_X_TRAIN='/content/drive/My Drive/АиТ/se/pred.csv'
PRED_X_VAL='/content/drive/My Drive/АиТ/se/pred_val.csv'

PRED2_X_TRAIN='/content/drive/My Drive/АиТ/ff_bert/pred2_bert_10.csv'
PRED2_X_VAL='/content/drive/My Drive/АиТ/ff_bert/pred2_bert_10_val.csv'

PRED3_X_TRAIN='/content/drive/My Drive/АиТ/ff_bert/pred3_bert_10.csv'
PRED3_X_VAL='/content/drive/My Drive/АиТ/ff_bert/pred3_bert_10_val.csv'

df = pd.read_csv(PATH_Y_TRAIN, delimiter=';', header=None)
train_labels=df[1].values[1:]
df = pd.read_csv(PATH_Y_TEST, delimiter=';', header=None)
test_labels=df[1].values[1:]
df = pd.read_csv(PATH_Y_VAL, delimiter=';', header=None)
val_labels=df[1].values[1:]

labels = np.unique(np.array(train_labels))
nums=np.arange(len(labels))
d = dict(zip(labels,nums))

train_labels=[d[c] for c in train_labels]
test_labels=[d[c] for c in list(test_labels)]
val_labels=[d[c] for c in list(val_labels)]

train_labels.extend(list(val_labels))

x_train=pd.read_csv(PATH_X_TRAIN, header=None)
x_test=pd.read_csv(PATH_X_VAL, header=None)



texts_train=x_train.values
texts_test=x_test.values

temp=texts_train[len(texts_train)-77:]
temp1=texts_train[:len(texts_train)-77]
print(texts_train.shape)
print(texts_test.shape)
print(temp.shape)
print(temp1.shape)
print('****')

texts_train=np.vstack((temp1,texts_test))
print(texts_train.shape)
texts_test=temp
print(texts_test.shape)

print(len(train_labels))
print(len(test_labels))

#context=1*************

pred_train=pd.read_csv(PRED_X_TRAIN, header=None).values
pred_test=pd.read_csv(PRED_X_VAL, header=None).values

temp=pred_train[len(pred_train)-77:]
temp1=pred_train[:len(pred_train)-77]

pred_train=np.vstack((temp1,pred_test))
pred_test=temp

post_train=pd.read_csv(POST_X_TRAIN, header=None).values
post_test=pd.read_csv(POST_X_VAL, header=None).values

temp=post_train[len(post_train)-77:]
temp1=post_train[:len(post_train)-77]

post_train=np.vstack((temp1,post_test))
post_test=temp


'''
pred_train=pd.read_csv(PRED_X_TRAIN, sep=';', index_col=0)
pred_test=pd.read_csv(PRED_X_TEST, sep=';', index_col=0)

post_train=pd.read_csv(POST_X_TRAIN, sep=';', index_col=0)
post_test=pd.read_csv(POST_X_TEST, sep=';', index_col=0)

pred2_train=pd.read_csv(PRED2_X_TRAIN, sep=';', index_col=0)
pred2_test=pd.read_csv(PRED2_X_TEST, sep=';', index_col=0)

post2_train=pd.read_csv(POST2_X_TRAIN, sep=';', index_col=0)
post2_test=pd.read_csv(POST2_X_TEST, sep=';', index_col=0)

pred3_train=pd.read_csv(PRED3_X_TRAIN, sep=';', index_col=0)
pred3_test=pd.read_csv(PRED3_X_TEST, sep=';', index_col=0)

post3_train=pd.read_csv(POST3_X_TRAIN, sep=';', index_col=0)
post3_test=pd.read_csv(POST3_X_TEST, sep=';', index_col=0)
'''

pred2_train.shape

#nm=np.hstack((np.hstack((pred2_train, pred_train)),texts_train))
#nm=np.hstack((np.hstack((nm, post_train)),post2_train))
#nm=np.hstack((np.hstack((pred3_train, nm)),post3_train))
#nm=np.hstack((np.hstack((pred4_train, nm)),post4_train))
nm=np.hstack((np.hstack((pred_train, texts_train)),post_train))

nm.shape

ros = RandomOverSampler(random_state=42)
#texts_train, classes_train = ros.fit_resample(texts_train, np.array(train_labels))
nm, classes_train = ros.fit_resample(nm, train_labels)
classes_train=np.array(classes_train)

pred_train,texts_train,post_train=np.hsplit(nm,(768, 1536))
#pred4_train, pred3_train, pred2_train, pred_train, texts_train, post_train, post2_train, post3_train, post4_train=np.hsplit(nm, (1, 2, 3, 4, 5, 6, 7, 8))

from keras import Sequential
import keras
from keras import *

inputs=Input(shape=(768,), name='input')
x=Dense(256, activation='tanh', name='fully_connected_256')(inputs)

x=Dense(32, activation='tanh', name='fully_connected_32')(x)
predictions=Dense(10, activation='softmax', name='output')(x)
model=Model(inputs=inputs, outputs=predictions)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
'''
model = Sequential()
model.add(Dense(256, input_dim=texts_train.shape[1], activation='tanh'))
#model.add(Dropout(0.2))
model.add(Dense(512, activation='tanh'))
#model.add(Dropout(0.8))
model.add(Dense(10, activation='softmax'))

model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
model.summary()
'''
from keras.utils import plot_model
plot_model(model, to_file='fnn.png')

from keras import Sequential

#1:0
inputs_x=Input(shape=(len(texts_train[0]),),name='input_pred')
inputs_y=Input(shape=(len(texts_train[0]),),name='input_texts')
inputs_z=Input(shape=(len(texts_train[0]),),name='input_post')

x=(Dense(32, activation='tanh', name='1'))(inputs_x)
y=(Dense(32, activation='tanh', name='2'))(inputs_y)
z=(Dense(32, activation='tanh', name='3'))(inputs_z)

x_m=Model(inputs=inputs_x, outputs=x)
y_m=Model(inputs=inputs_y, outputs=y)
z_m=Model(inputs=inputs_z, outputs=z)

combined1=concatenate([x_m.output, y_m.output, z_m.output], name='concat')

x1=(Dense(32, activation='tanh', name='11'))(x)
y1=(Dense(32, activation='tanh', name='21'))(combined1)
z1=(Dense(32, activation='tanh', name='31'))(z)

x1=Model(inputs=inputs_x, outputs=x1)
y1=Model(inputs=[inputs_x,inputs_y,inputs_z], outputs=y1)
z1=Model(inputs=inputs_z, outputs=z1)

combined=concatenate([x1.output, y1.output, z1.output], name='concat1')

ex=Dense(32, activation='tanh', name='fully_connected_32')(combined)
predictions=Dense(10, activation='softmax', name='output')(combined1)
model=Model(inputs=[inputs_x, inputs_y, inputs_z], outputs=predictions)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()


'''
model = Sequential()
model.add(Dense(100, input_dim=texts_train.shape[1], activation='tanh'))
model.add(Dropout(0.2))
model.add(Dense(100, activation='tanh'))
model.add(Dropout(0.8))
model.add(Dense(10, activation='softmax'))

model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
'''
model.summary()

import keras
from keras import *
classes_train = keras.utils.to_categorical(np.array(classes_train),10)
val_labels = keras.utils.to_categorical(np.array(test_labels),10)

#выбор модели

def build_model():
  inputs_x=Input(shape=(len(texts_train[0]),),name='input_pred')
  inputs_y=Input(shape=(len(texts_train[0]),),name='input_texts')
  inputs_z=Input(shape=(len(texts_train[0]),),name='input_post')

  x=(Dense(256, activation='tanh', name='11'))(inputs_x)
  y=(Dense(256, activation='tanh', name='22'))(inputs_y)
  z=(Dense(256, activation='tanh', name='33'))(inputs_z)

  x1=Model(inputs=inputs_x, outputs=x)
  y1=Model(inputs=inputs_y, outputs=y)
  z1=Model(inputs=inputs_z, outputs=z)

  combined=concatenate([x1.output, y1.output, z1.output], name='concat1')

  ex=Dense(32, activation='tanh', name='fully_connected_32')(combined)
  #predictions=Dense(10, activation='softmax', name='output')(ex)
  predictions=Dense(10, activation='softmax', name='output')(combined)
  model=Model(inputs=[inputs_x, inputs_y, inputs_z], outputs=predictions)
  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
  from keras.utils import plot_model
  plot_model(model, to_file='fnn.png')
  return model

from sklearn.metrics import f1_score, recall_score, precision_score
best_f1=0
best_prec=0
best_recall=0

for j in range(1):
  model=build_model()
  for i in range(5):
    model.fit([pred_train,texts_train,post_train], np.array(classes_train), epochs=1, verbose=0, validation_data=([pred_test,texts_test,post_test], np.array(val_labels)))
    predict = np.argmax(model.predict([pred_test,texts_test,post_test]), axis=1)
    answer = np.argmax(val_labels, axis=1)
    f1=f1_score(predict, answer, average="macro")*100
    prec=precision_score(predict, answer, average="macro")*100
    recall=recall_score(predict, answer, average="macro")*100
    if f1>best_f1:
      best_f1=f1
      best_prec=prec
      best_recall=recall
      print(best_f1)
      print(str(j)+' '+str(i))

print('*******************')
print('best model')
print(best_f1)
print(best_prec)
print(best_recall)

predict

answer

for i, elem in enumerate(predict)
:
  if answer[i]!=predict[i]:
    print(texts_test)

model.fit([pred_train,texts_train,post_train], np.array(classes_train), epochs=10, verbose=1, validation_data=([pred_test,texts_test,post_test], np.array(val_labels)))

from sklearn.metrics import f1_score, recall_score, precision_score
predict = np.argmax(model.predict([pred_test,texts_test,post_test]), axis=1)
answer = np.argmax(val_labels, axis=1)
print('F1-score: %f' % (f1_score(predict, answer, average="macro")*100))
print('Precision: %f' % (precision_score(predict, answer, average="macro")*100))
print('Recall: %f' % (recall_score(predict, answer, average="macro")*100))

from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, concatenate
from keras.optimizers import RMSprop
from keras.layers import Bidirectional
'''
#без контекста

inputs=Input(shape=(maxSequenceLength,), name='input', dtype='int64')
x=Embedding(vocab_size, 300, name='word2vec', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs)

x=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64')(x)

x=Dense(32, activation='tanh', name='fully_connected_32')(x)
predictions=Dense(num_classes, activation='softmax', name='output')(x)
model=Model(inputs=inputs, outputs=predictions)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
'''
'''
#1:0
inputs_x=Input(shape=(maxSequenceLength,), name='input_pred', dtype='int64')
inputs_y=Input(shape=(maxSequenceLength,), name='input', dtype='int64')
inputs_z=Input(shape=(maxSequenceLength,), name='input_post', dtype='int64')

x=Embedding(vocab_size, 300, name='word2vec_pred', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x)
y=Embedding(vocab_size, 300, name='word2vec', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_y)
z=Embedding(vocab_size, 300, name='word2vec_post', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z)

x=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred')(x)
y=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64')(y)
z=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post')(z)

x=Model(inputs=inputs_x, outputs=x)
y=Model(inputs=inputs_y, outputs=y)
z=Model(inputs=inputs_z, outputs=z)

combined=concatenate([x.output, y.output, z.output], name='concat')

ex=Dense(32, activation='tanh', name='fully_connected_32')(combined)
predictions=Dense(num_classes, activation='softmax', name='output')(ex)
model=Model(inputs=[inputs_x, inputs_y, inputs_z], outputs=predictions)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
'''
'''
#0:1
inputs_x=Input(shape=(maxSequenceLength,), name='input_pred', dtype='int64')
inputs_y=Input(shape=(maxSequenceLength,), name='input', dtype='int64')
inputs_z=Input(shape=(maxSequenceLength,), name='input_post', dtype='int64')

x=Embedding(vocab_size, 300, name='word2vec_pred', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x)
y=Embedding(vocab_size, 300, name='word2vec', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_y)
z=Embedding(vocab_size, 300, name='word2vec_post', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z)

x=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred')(x)
y=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64')(y)
z=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post')(z)

x=Dense(32, activation='tanh', name='fully_connected_32_pred')(x)
y=Dense(32, activation='tanh', name='fully_connected_32')(y)
z=Dense(32, activation='tanh', name='fully_connected_32_post')(z)

x=Model(inputs=inputs_x, outputs=x)
y=Model(inputs=inputs_y, outputs=y)
z=Model(inputs=inputs_z, outputs=z)

combined=concatenate([x.output, y.output, z.output], name='concat')

predictions=Dense(num_classes, activation='softmax', name='output')(combined)
model=Model(inputs=[inputs_x, inputs_y, inputs_z], outputs=predictions)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()


'''
'''
#1:1
inputs_x=Input(shape=(maxSequenceLength,), name='input_pred', dtype='int64')
inputs_y=Input(shape=(maxSequenceLength,), name='input', dtype='int64')
inputs_z=Input(shape=(maxSequenceLength,), name='input_post', dtype='int64')

x=Embedding(vocab_size, 300, name='word2vec_pred', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x)
y=Embedding(vocab_size, 300, name='word2vec', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_y)
z=Embedding(vocab_size, 300, name='word2vec_post', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z)

x=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred')(x)
y=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64')(y)
z=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post')(z)

x_m=Model(inputs=inputs_x, outputs=x)
y_m=Model(inputs=inputs_y, outputs=y)
z_m=Model(inputs=inputs_z, outputs=z)

combined=concatenate([x_m.output, y_m.output, z_m.output], name='concat_first')

x=Dense(32, activation='tanh', name='fully_connected_32_pred')(x)
ex=Dense(32, activation='tanh', name='fully_connected_32')(combined)
z=Dense(32, activation='tanh', name='fully_connected_32_post')(z)

x_m1=Model(inputs=inputs_x, outputs=x)
y_m1=Model(inputs=[inputs_x, inputs_y, inputs_z], outputs=ex)
z_m1=Model(inputs=inputs_z, outputs=z)

combined1=concatenate([x_m1.output, y_m1.output, z_m1.output], name='concat_second')

predictions=Dense(num_classes, activation='softmax', name='output')(combined1)
model=Model(inputs=[inputs_x, inputs_y, inputs_z], outputs=predictions)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
'''
'''
#2:0
inputs_x1=Input(shape=(maxSequenceLength,), name='input_pred2', dtype='int64')
inputs_x=Input(shape=(maxSequenceLength,), name='input_pred1', dtype='int64')
inputs_y=Input(shape=(maxSequenceLength,), name='input', dtype='int64')
inputs_z=Input(shape=(maxSequenceLength,), name='input_post1', dtype='int64')
inputs_z1=Input(shape=(maxSequenceLength,), name='input_post2', dtype='int64')

x1=Embedding(vocab_size, 300, name='word2vec_pred2', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x1)
x=Embedding(vocab_size, 300, name='word2vec_pred1', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x)
y=Embedding(vocab_size, 300, name='word2vec', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_y)
z=Embedding(vocab_size, 300, name='word2vec_post1', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z)
z1=Embedding(vocab_size, 300, name='word2vec_post2', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z1)

x1=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred2')(x1)
x=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred1')(x)
y=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64')(y)
z=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post1')(z)
z1=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post2')(z1)

x1=Model(inputs=inputs_x1, outputs=x1)
x=Model(inputs=inputs_x, outputs=x)
y=Model(inputs=inputs_y, outputs=y)
z=Model(inputs=inputs_z, outputs=z)
z1=Model(inputs=inputs_z1, outputs=z1)

combined=concatenate([x1.output, x.output, y.output, z.output, z1.output], name='concat')

ex=Dense(32, activation='tanh', name='fully_connected_32')(combined)
predictions=Dense(num_classes, activation='softmax', name='output')(ex)
model=Model(inputs=[inputs_x1, inputs_x, inputs_y, inputs_z, inputs_z1], outputs=predictions)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
'''
'''
#0:2
inputs_x1=Input(shape=(maxSequenceLength,), name='input_pred2', dtype='int64')
inputs_x=Input(shape=(maxSequenceLength,), name='input_pred1', dtype='int64')
inputs_y=Input(shape=(maxSequenceLength,), name='input', dtype='int64')
inputs_z=Input(shape=(maxSequenceLength,), name='input_post1', dtype='int64')
inputs_z1=Input(shape=(maxSequenceLength,), name='input_post2', dtype='int64')

x1=Embedding(vocab_size, 300, name='word2vec_pred2', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x1)
x=Embedding(vocab_size, 300, name='word2vec_pred1', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x)
y=Embedding(vocab_size, 300, name='word2vec', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_y)
z=Embedding(vocab_size, 300, name='word2vec_post1', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z)
z1=Embedding(vocab_size, 300, name='word2vec_post2', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z1)

x1=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred2')(x1)
x=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred1')(x)
y=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64')(y)
z=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post1')(z)
z1=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post2')(z1)

x1=Dense(32, activation='tanh', name='fully_connected_32_pred2')(x1)
x=Dense(32, activation='tanh', name='fully_connected_32_pred1')(x)
y=Dense(32, activation='tanh', name='fully_connected_32')(y)
z=Dense(32, activation='tanh', name='fully_connected_32_post1')(z)
z1=Dense(32, activation='tanh', name='fully_connected_32_post2')(z1)

x1=Model(inputs=inputs_x1, outputs=x1)
x=Model(inputs=inputs_x, outputs=x)
y=Model(inputs=inputs_y, outputs=y)
z=Model(inputs=inputs_z, outputs=z)
z1=Model(inputs=inputs_z1, outputs=z1)

combined=concatenate([x1.output, x.output, y.output, z.output, z1.output], name='concat')

predictions=Dense(num_classes, activation='softmax', name='output')(combined)
model=Model(inputs=[inputs_x1, inputs_x, inputs_y, inputs_z, inputs_z1], outputs=predictions)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
'''
'''
#2:2
inputs_x1=Input(shape=(maxSequenceLength,), name='input_pred2', dtype='int64')
inputs_x=Input(shape=(maxSequenceLength,), name='input_pred1', dtype='int64')
inputs_y=Input(shape=(maxSequenceLength,), name='input', dtype='int64')
inputs_z=Input(shape=(maxSequenceLength,), name='input_post1', dtype='int64')
inputs_z1=Input(shape=(maxSequenceLength,), name='input_post2', dtype='int64')

x1=Embedding(vocab_size, 300, name='word2vec_pred2', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x1)
x=Embedding(vocab_size, 300, name='word2vec_pred1', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x)
y=Embedding(vocab_size, 300, name='word2vec', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_y)
z=Embedding(vocab_size, 300, name='word2vec_post1', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z)
z1=Embedding(vocab_size, 300, name='word2vec_post2', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z1)

x1=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred2')(x1)
x=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred1')(x)
y=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64')(y)
z=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post1')(z)
z1=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post2')(z1)

x_m_1=Model(inputs=inputs_x1, outputs=x1)
x_m=Model(inputs=inputs_x, outputs=x)
y_m=Model(inputs=inputs_y, outputs=y)
z_m=Model(inputs=inputs_z, outputs=z)
z_m_1=Model(inputs=inputs_z1, outputs=z1)

combined=concatenate([x_m_1.output, x_m.output, y_m.output, z_m.output, z_m_1.output], name='concat_first')

x1=Dense(32, activation='tanh', name='fully_connected_32_pred2')(x1)
x=Dense(32, activation='tanh', name='fully_connected_32_pred1')(x)
ex=Dense(32, activation='tanh', name='fully_connected_32')(combined)
z=Dense(32, activation='tanh', name='fully_connected_32_post1')(z)
z1=Dense(32, activation='tanh', name='fully_connected_32_post2')(z1)

x_m1_1=Model(inputs=inputs_x1, outputs=x1)
x_m1=Model(inputs=inputs_x, outputs=x)
y_m1=Model(inputs=[inputs_x1, inputs_x, inputs_y, inputs_z, inputs_z1], outputs=ex)
z_m1=Model(inputs=inputs_z, outputs=z)
z_m1_1=Model(inputs=inputs_z1, outputs=z1)

combined1=concatenate([x_m1_1.output, x_m1.output, y_m1.output, z_m1.output, z_m1_1.output], name='concat_second')

predictions=Dense(num_classes, activation='softmax', name='output')(combined1)
model=Model(inputs=[inputs_x1, inputs_x, inputs_y, inputs_z, inputs_z1], outputs=predictions)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
'''
'''
#3:0
inputs_x2=Input(shape=(maxSequenceLength,), name='input_pred3', dtype='int64')
inputs_x1=Input(shape=(maxSequenceLength,), name='input_pred2', dtype='int64')
inputs_x=Input(shape=(maxSequenceLength,), name='input_pred1', dtype='int64')
inputs_y=Input(shape=(maxSequenceLength,), name='input', dtype='int64')
inputs_z=Input(shape=(maxSequenceLength,), name='input_post1', dtype='int64')
inputs_z1=Input(shape=(maxSequenceLength,), name='input_post2', dtype='int64')
inputs_z2=Input(shape=(maxSequenceLength,), name='input_post3', dtype='int64')

x2=Embedding(vocab_size, 300, name='word2vec_pred3', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x2)
x1=Embedding(vocab_size, 300, name='word2vec_pred2', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x1)
x=Embedding(vocab_size, 300, name='word2vec_pred1', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x)
y=Embedding(vocab_size, 300, name='word2vec', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_y)
z=Embedding(vocab_size, 300, name='word2vec_post1', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z)
z1=Embedding(vocab_size, 300, name='word2vec_post2', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z1)
z2=Embedding(vocab_size, 300, name='word2vec_post3', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z2)

x2=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred3')(x2)
x1=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred2')(x1)
x=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred1')(x)
y=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64')(y)
z=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post1')(z)
z1=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post2')(z1)
z2=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post3')(z2)

x2=Model(inputs=inputs_x2, outputs=x2)
x1=Model(inputs=inputs_x1, outputs=x1)
x=Model(inputs=inputs_x, outputs=x)
y=Model(inputs=inputs_y, outputs=y)
z=Model(inputs=inputs_z, outputs=z)
z1=Model(inputs=inputs_z1, outputs=z1)
z2=Model(inputs=inputs_z2, outputs=z2)

combined=concatenate([x2.output, x1.output, x.output, y.output, z.output, z1.output, z2.output], name='concat')

ex=Dense(32, activation='tanh', name='fully_connected_32')(combined)
predictions=Dense(num_classes, activation='softmax', name='output')(ex)
model=Model(inputs=[inputs_x2, inputs_x1, inputs_x, inputs_y, inputs_z, inputs_z1, inputs_z2], outputs=predictions)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
'''
'''
#0:3
inputs_x2=Input(shape=(maxSequenceLength,), name='input_pred3', dtype='int64')
inputs_x1=Input(shape=(maxSequenceLength,), name='input_pred2', dtype='int64')
inputs_x=Input(shape=(maxSequenceLength,), name='input_pred1', dtype='int64')
inputs_y=Input(shape=(maxSequenceLength,), name='input', dtype='int64')
inputs_z=Input(shape=(maxSequenceLength,), name='input_post1', dtype='int64')
inputs_z1=Input(shape=(maxSequenceLength,), name='input_post2', dtype='int64')
inputs_z2=Input(shape=(maxSequenceLength,), name='input_post3', dtype='int64')

x2=Embedding(vocab_size, 300, name='word2vec_pred3', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x2)
x1=Embedding(vocab_size, 300, name='word2vec_pred2', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x1)
x=Embedding(vocab_size, 300, name='word2vec_pred1', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x)
y=Embedding(vocab_size, 300, name='word2vec', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_y)
z=Embedding(vocab_size, 300, name='word2vec_post1', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z)
z1=Embedding(vocab_size, 300, name='word2vec_post2', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z1)
z2=Embedding(vocab_size, 300, name='word2vec_post3', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z2)

x2=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred3')(x2)
x1=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred2')(x1)
x=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred1')(x)
y=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64')(y)
z=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post1')(z)
z1=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post2')(z1)
z2=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post3')(z2)

x2=Dense(32, activation='tanh', name='fully_connected_32_pred3')(x2)
x1=Dense(32, activation='tanh', name='fully_connected_32_pred2')(x1)
x=Dense(32, activation='tanh', name='fully_connected_32_pred1')(x)
y=Dense(32, activation='tanh', name='fully_connected_32')(y)
z=Dense(32, activation='tanh', name='fully_connected_32_post1')(z)
z1=Dense(32, activation='tanh', name='fully_connected_32_post2')(z1)
z2=Dense(32, activation='tanh', name='fully_connected_32_post3')(z2)

x2=Model(inputs=inputs_x2, outputs=x2)
x1=Model(inputs=inputs_x1, outputs=x1)
x=Model(inputs=inputs_x, outputs=x)
y=Model(inputs=inputs_y, outputs=y)
z=Model(inputs=inputs_z, outputs=z)
z1=Model(inputs=inputs_z1, outputs=z1)
z2=Model(inputs=inputs_z2, outputs=z2)

combined=concatenate([x2.output, x1.output, x.output, y.output, z.output, z1.output, z2.output], name='concat')

predictions=Dense(num_classes, activation='softmax', name='output')(combined)
model=Model(inputs=[inputs_x2, inputs_x1, inputs_x, inputs_y, inputs_z, inputs_z1, inputs_z2], outputs=predictions)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
'''
'''
#3_3
inputs_x2=Input(shape=(maxSequenceLength,), name='input_pred3', dtype='int64')
inputs_x1=Input(shape=(maxSequenceLength,), name='input_pred2', dtype='int64')
inputs_x=Input(shape=(maxSequenceLength,), name='input_pred1', dtype='int64')
inputs_y=Input(shape=(maxSequenceLength,), name='input', dtype='int64')
inputs_z=Input(shape=(maxSequenceLength,), name='input_post1', dtype='int64')
inputs_z1=Input(shape=(maxSequenceLength,), name='input_post2', dtype='int64')
inputs_z2=Input(shape=(maxSequenceLength,), name='input_post3', dtype='int64')

x2=Embedding(vocab_size, 300, name='word2vec_pred3', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x2)
x1=Embedding(vocab_size, 300, name='word2vec_pred2', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x1)
x=Embedding(vocab_size, 300, name='word2vec_pred1', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x)
y=Embedding(vocab_size, 300, name='word2vec', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_y)
z=Embedding(vocab_size, 300, name='word2vec_post1', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z)
z1=Embedding(vocab_size, 300, name='word2vec_post2', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z1)
z2=Embedding(vocab_size, 300, name='word2vec_post3', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z2)

x2=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred3')(x2)
x1=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred2')(x1)
x=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred1')(x)
y=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64')(y)
z=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post1')(z)
z1=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post2')(z1)
z2=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post3')(z2)

x_m_2=Model(inputs=inputs_x2, outputs=x2)
x_m_1=Model(inputs=inputs_x1, outputs=x1)
x_m=Model(inputs=inputs_x, outputs=x)
y_m=Model(inputs=inputs_y, outputs=y)
z_m=Model(inputs=inputs_z, outputs=z)
z_m_1=Model(inputs=inputs_z1, outputs=z1)
z_m_2=Model(inputs=inputs_z2, outputs=z2)

combined=concatenate([x_m_2.output, x_m_1.output, x_m.output, y_m.output, z_m.output, z_m_1.output, z_m_2.output], name='concat_first')

x2=Dense(32, activation='tanh', name='fully_connected_32_pred3')(x2)
x1=Dense(32, activation='tanh', name='fully_connected_32_pred2')(x1)
x=Dense(32, activation='tanh', name='fully_connected_32_pred1')(x)
y=Dense(32, activation='tanh', name='fully_connected_32')(y)
z=Dense(32, activation='tanh', name='fully_connected_32_post1')(z)
z1=Dense(32, activation='tanh', name='fully_connected_32_post2')(z1)
z2=Dense(32, activation='tanh', name='fully_connected_32_post3')(z2)

x_m1_2=Model(inputs=inputs_x2, outputs=x2)
x_m1_1=Model(inputs=inputs_x1, outputs=x1)
x_m1=Model(inputs=inputs_x, outputs=x)
y_m1=Model(inputs=[inputs_x2, inputs_x1, inputs_x, inputs_y, inputs_z, inputs_z1, inputs_z2], outputs=y)
z_m1=Model(inputs=inputs_z, outputs=z)
z_m1_1=Model(inputs=inputs_z1, outputs=z1)
z_m1_2=Model(inputs=inputs_z2, outputs=z2)

combined1=concatenate([x_m1_2.output, x_m1_1.output, x_m1.output, y_m1.output, z_m1.output, z_m1_1.output, z_m1_2.output], name='concat_second')

predictions=Dense(num_classes, activation='softmax', name='output')(combined1)
model=Model(inputs=[inputs_x2, inputs_x1, inputs_x, inputs_y, inputs_z, inputs_z1, inputs_z2], outputs=predictions)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
'''
#'''
#3:0
inputs_x3=Input(shape=(maxSequenceLength,), name='input_pred4', dtype='int64')
inputs_x2=Input(shape=(maxSequenceLength,), name='input_pred3', dtype='int64')
inputs_x1=Input(shape=(maxSequenceLength,), name='input_pred2', dtype='int64')
inputs_x=Input(shape=(maxSequenceLength,), name='input_pred1', dtype='int64')
inputs_y=Input(shape=(maxSequenceLength,), name='input', dtype='int64')
inputs_z=Input(shape=(maxSequenceLength,), name='input_post1', dtype='int64')
inputs_z1=Input(shape=(maxSequenceLength,), name='input_post2', dtype='int64')
inputs_z2=Input(shape=(maxSequenceLength,), name='input_post3', dtype='int64')
inputs_z3=Input(shape=(maxSequenceLength,), name='input_post4', dtype='int64')

x3=Embedding(vocab_size, 300, name='word2vec_pred4', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x3)
x2=Embedding(vocab_size, 300, name='word2vec_pred3', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x2)
x1=Embedding(vocab_size, 300, name='word2vec_pred2', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x1)
x=Embedding(vocab_size, 300, name='word2vec_pred1', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_x)
y=Embedding(vocab_size, 300, name='word2vec', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_y)
z=Embedding(vocab_size, 300, name='word2vec_post1', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z)
z1=Embedding(vocab_size, 300, name='word2vec_post2', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z1)
z2=Embedding(vocab_size, 300, name='word2vec_post3', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z2)
z3=Embedding(vocab_size, 300, name='word2vec_post4', weights=[embedding_matrix], input_length=maxSequenceLength, trainable=False)(inputs_z3)

x3=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred4')(x3)
x2=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred3')(x2)
x1=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred2')(x1)
x=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_pred1')(x)
y=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64')(y)
z=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post1')(z)
z1=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post2')(z1)
z2=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post3')(z2)
z3=LSTM(64, dropout=0.5, recurrent_dropout=0.2, name='lstm_64_post4')(z3)

x3=Model(inputs=inputs_x3, outputs=x3)
x2=Model(inputs=inputs_x2, outputs=x2)
x1=Model(inputs=inputs_x1, outputs=x1)
x=Model(inputs=inputs_x, outputs=x)
y=Model(inputs=inputs_y, outputs=y)
z=Model(inputs=inputs_z, outputs=z)
z1=Model(inputs=inputs_z1, outputs=z1)
z2=Model(inputs=inputs_z2, outputs=z2)
z3=Model(inputs=inputs_z3, outputs=z3)

combined=concatenate([x3.output, x2.output, x1.output, x.output, y.output, z.output, z1.output, z2.output, z3.output], name='concat')

ex=Dense(32, activation='tanh', name='fully_connected_32')(combined)
predictions=Dense(num_classes, activation='softmax', name='output')(ex)
model=Model(inputs=[inputs_x3, inputs_x2, inputs_x1, inputs_x, inputs_y, inputs_z, inputs_z1, inputs_z2, inputs_z3], outputs=predictions)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
#'''

from keras.utils import plot_model
plot_model(model, to_file='0_3.png')

for i in range(20):
    print(i)
    history = model.fit([padded_docs_pr4_train, padded_docs_pr3_train, padded_docs_pr2_train, padded_docs_pr_train, padded_docs_train, padded_docs_po_train, padded_docs_po2_train, padded_docs_po3_train, padded_docs_po4_train], classes_train, epochs = 1, verbose=2, validation_data=([padded_docs_pr4_test, padded_docs_pr3_test, padded_docs_pr2_test, padded_docs_pr_test, padded_docs_test, padded_docs_po_test, padded_docs_po2_test, padded_docs_po3_test, padded_docs_po4_test], classes_test))
    predict = np.argmax(model.predict([padded_docs_pr4_val, padded_docs_pr3_val, padded_docs_pr2_val, padded_docs_pr_val, padded_docs_val, padded_docs_po_val, padded_docs_po2_val, padded_docs_po3_val, padded_docs_po4_val]), axis=1)
    answer = np.argmax(classes_val, axis=1)
    print('F1-score: %f' % (f1_score(predict, answer, average="macro")*100))

for i in range(20):
    print(i)
    history = model.fit([padded_docs_pr3_train, padded_docs_pr2_train, padded_docs_pr_train, padded_docs_train, padded_docs_po_train, padded_docs_po2_train, padded_docs_po3_train], classes_train, epochs = 1, verbose=2, validation_data=([padded_docs_pr3_test, padded_docs_pr2_test, padded_docs_pr_test, padded_docs_test, padded_docs_po_test, padded_docs_po2_test, padded_docs_po3_test], classes_test))
    predict = np.argmax(model.predict([padded_docs_pr3_val, padded_docs_pr2_val, padded_docs_pr_val, padded_docs_val, padded_docs_po_val, padded_docs_po2_val, padded_docs_po3_val]), axis=1)
    answer = np.argmax(classes_val, axis=1)
    print('F1-score: %f' % (f1_score(predict, answer, average="macro")*100))

for i in range(20):
    print(i)
    history = model.fit([padded_docs_pr2_train, padded_docs_pr_train, padded_docs_train, padded_docs_po_train, padded_docs_po2_train], classes_train, epochs = 1, verbose=2, validation_data=([padded_docs_pr2_test, padded_docs_pr_test, padded_docs_test, padded_docs_po_test, padded_docs_po2_test], classes_test))
    predict = np.argmax(model.predict([padded_docs_pr2_val, padded_docs_pr_val, padded_docs_val, padded_docs_po_val, padded_docs_po2_val]), axis=1)
    answer = np.argmax(classes_val, axis=1)
    print('F1-score: %f' % (f1_score(predict, answer, average="macro")*100))

### from sklearn.metrics import f1_score
for i in range(20):
    print(i)
    history = model.fit([padded_docs_pr_train, padded_docs_train, padded_docs_po_train], classes_train, epochs = 1, verbose=2, validation_data=([padded_docs_pr_test, padded_docs_test, padded_docs_po_test], classes_test))
    predict = np.argmax(model.predict([padded_docs_pr_val, padded_docs_val, padded_docs_po_val]), axis=1)
    answer = np.argmax(classes_val, axis=1)
    print('F1-score: %f' % (f1_score(predict, answer, average="macro")*100))

### from sklearn.metrics import f1_score
for i in range(20):
    print(i)
    history = model.fit(padded_docs_train, classes_train, epochs = 1, verbose=2, validation_data=(padded_docs_test, classes_test))
    predict = np.argmax(model.predict(padded_docs_val), axis=1)
    answer = np.argmax(classes_val, axis=1)
    print('F1-score: %f' % (f1_score(predict, answer, average="macro")*100))



